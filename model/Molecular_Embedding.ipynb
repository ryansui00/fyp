{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "from mol2vec.features import mol2alt_sentence, mol2sentence, MolSentence, DfVec, sentences2vec\n",
    "from gensim.models import word2vec, keyedvectors\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset\n",
    "df = pd.read_excel(\"training_data.xlsx\")\n",
    "\n",
    "#Generate molecules\n",
    "df['ROMol'] = df.apply(lambda x: Chem.MolFromSmiles(x['SMILES']), axis=1)\n",
    "\n",
    "#Load pre-trained mol2vec model\n",
    "model = word2vec.Word2Vec.load('mol2vec-master/examples/models/model_300dim.pkl')\n",
    "\n",
    "df['sentence'] = df.apply(lambda x: MolSentence(mol2alt_sentence(x['ROMol'], 1)), axis=1)\n",
    "df['mol2vec'] = [DfVec(x) for x in sentences2vec(df['sentence'], model, unseen='UNK')]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['SMILES','mol2vec']],df['value'],test_size=0.2,random_state=42)\n",
    "\n",
    "X_train_vec = np.array([x.vec for x in X_train['mol2vec']])\n",
    "X_test_vec = np.array([x.vec for x in X_test['mol2vec']])\n",
    "\n",
    "y_train = np.array(y_train.astype(float))\n",
    "y_test = np.array(y_test.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning of RF model\n",
    "\n",
    "#Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "#Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt',1.0]\n",
    "#Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "#Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "#Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "#Method of selecting samples for training each tree\n",
    "bootstrap = [True, False] \n",
    "#Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "#Initialise model\n",
    "rf = RandomForestRegressor()\n",
    "#Implement random search of parameters with 5-fold validation\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=1, random_state=0, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train_vec, y_train)\n",
    "\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement grid search hyperparameter tuning\n",
    "\n",
    "param_grid = {'n_estimators' : [1600,1700,1800],\n",
    "'max_features' : ['sqrt'],\n",
    "'max_depth' : [70],\n",
    "'min_samples_split' : [2,3,4],\n",
    "'min_samples_leaf' : [1],\n",
    "'bootstrap' : [False]\n",
    "}\n",
    "\n",
    "#Initialise model\n",
    "rf = RandomForestRegressor()\n",
    "#Implment grid search\n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 1)\n",
    "rf_grid.fit(X_train_vec,y_train)\n",
    "print(rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform 5-fold cross validation with parameters\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "for train,test in kf.split(X_train_vec,y_train):\n",
    "    regr = RandomForestRegressor(n_estimators = 1800, max_depth = 70, min_samples_split = 2, max_features='sqrt', min_samples_leaf = 1, bootstrap = False, random_state = 0)\n",
    "    regr.fit(X_train_vec[train],y_train[train])\n",
    "    y_pred = regr.predict(X_train_vec[test])\n",
    "    y_true = np.array(y_train[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #plot data\n",
    "    # y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "    # plt.scatter(y_true,y_pred)\n",
    "    # plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "    # plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "    # plt.xlabel(\"Actual Tg ($^\\circ$C)\")\n",
    "    # plt.ylabel(\"Predicted Tg ($^\\circ$C)\")\n",
    "    # plt.show()\n",
    "    # del regr\n",
    "\n",
    "print(r2)\n",
    "print(mae)\n",
    "print(rmse)\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "regr.fit(X_train_vec,y_train)\n",
    "y_pred = regr.predict(X_test_vec)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "# plt.annotate(\"r2 = {:.4f}\".format(r2_score(y_true, y_pred)), (7.5, 1))\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning of SVM model\n",
    "\n",
    "param_grid = {'kernel' : ['linear'],\n",
    "              'C' : [0.1, 1, 10],\n",
    "              'gamma' : [0.001, 0.01, 0.01,'scale','auto'],\n",
    "              'epsilon' : [0.001, 0.01, 0.1, 1]\n",
    "              }\n",
    "\n",
    "#Initialise model\n",
    "svr = svm.SVR()\n",
    "#Perform grid search\n",
    "svr_grid = GridSearchCV(estimator = svr, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 1)\n",
    "#Fit grid search model\n",
    "svr_grid.fit(X_train_vec,y_train)\n",
    "\n",
    "print(svr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further tuning of SVM model\n",
    "\n",
    "param_grid = {'kernel' : ['linear','rbf','poly','sigmoid'],\n",
    "              'C' : [0.1],\n",
    "              'gamma' : [0.001, 'auto'],\n",
    "              'epsilon' : [0.01, 0.1, 1]\n",
    "              }\n",
    "\n",
    "#Initialise model\n",
    "svr = svm.SVR()\n",
    "#Perform grid search\n",
    "svr_grid = GridSearchCV(estimator = svr, param_grid = param_grid, cv = 5, n_jobs = -1, verbose = 1)\n",
    "#Fit grid search model\n",
    "svr_grid.fit(X_train_vec,y_train)\n",
    "\n",
    "print(svr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform 5-fold cross validation with parameters\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "for train,test in kf.split(X_train_vec,y_train):\n",
    "    regr = svm.SVR(kernel=\"linear\", C=0.1, tol=0.001, gamma=0.001,epsilon=0.1)\n",
    "    regr.fit(X_train_vec[train],y_train[train])\n",
    "    y_pred = regr.predict(X_train_vec[test])\n",
    "    y_true = np.array(y_train[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #plot data\n",
    "    # y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "    # plt.scatter(y_true,y_pred)\n",
    "    # plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "    # plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "    # plt.xlabel(\"Actual Tg ($^\\circ$C)\")\n",
    "    # plt.ylabel(\"Predicted Tg ($^\\circ$C)\")\n",
    "    # plt.show()\n",
    "   \n",
    "\n",
    "print(r2)\n",
    "print(mae)\n",
    "print(rmse)\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test model on unseen data\n",
    "regr.fit(X_train_vec,y_train)\n",
    "y_pred = regr.predict(X_test_vec)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Regression (GPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import GPR model\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, RBF\n",
    "\n",
    "#initialise kernel\n",
    "kernel = WhiteKernel(noise_level=0.1) + RBF(length_scale=10)\n",
    "\n",
    "#perform 5-fold cross validation for MF\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "for train,test in kf.split(X_train_vec,y_train):\n",
    "    regr = GaussianProcessRegressor(kernel=kernel,random_state=0)\n",
    "    regr.fit(X_train_vec[train],y_train[train])\n",
    "    y_pred = regr.predict(X_train_vec[test])\n",
    "    y_true = np.array(y_train[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #plot data\n",
    "    # y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "    # plt.scatter(y_true,y_pred)\n",
    "    # plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "    # plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "    # plt.xlabel(\"Actual Tg ($^\\circ$C)\")\n",
    "    # plt.ylabel(\"Predicted Tg ($^\\circ$C)\")\n",
    "    # plt.show()\n",
    "    # del regr\n",
    "\n",
    "print(r2)\n",
    "print(mae)\n",
    "print(rmse)\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "regr.fit(X_train_vec,y_train)\n",
    "y_pred = regr.predict(X_test_vec)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
    "\n",
    "#define parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "X_train_vec = X_train_vec.reshape(X_train_vec.shape[0],X_train_vec.shape[1],1)\n",
    "X_test_vec = X_test_vec.reshape(X_test_vec.shape[0],X_test_vec.shape[1],1)\n",
    "\n",
    "#implement model for MF\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(300, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "#compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#run k-cross validation\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "train_loss, val_loss = [], []\n",
    "for train,test in kf.split(X_train_vec,y_train):\n",
    "    hist = model.fit(X_train_vec[train],y_train[train],batch_size,epochs,verbose=0,validation_data=(X_train_vec[test], y_train[test]))\n",
    "    y_pred = model.predict(X_train_vec[test])\n",
    "    y_true = np.array(y_train[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #record loss values\n",
    "    train_loss.append(hist.history['loss'])\n",
    "    val_loss.append(hist.history['val_loss'])\n",
    "\n",
    "#learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.mean(train_loss, axis=0), label='Training Loss')\n",
    "plt.plot(np.mean(val_loss, axis=0), label='Validation Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "model.fit(X_train_vec,y_train,batch_size,epochs,verbose=0)\n",
    "y_pred = model.predict(X_test_vec)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "#define parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "X_train_vec = X_train_vec.reshape(X_train_vec.shape[0],1,X_train_vec.shape[1])\n",
    "X_test_vec = X_test_vec.reshape(X_test_vec.shape[0],1,X_test_vec.shape[1]) \n",
    "\n",
    "#implment model for MF\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1, 300)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "#compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#run k-cross validation\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "train_loss, val_loss = [], []\n",
    "for train,test in kf.split(X_train_vec,y_train):\n",
    "    hist = model.fit(X_train_vec[train],y_train[train],batch_size,epochs,verbose=0,validation_data=(X_train_vec[test], y_train[test]))\n",
    "    y_pred = model.predict(X_train_vec[test])\n",
    "    y_true = np.array(y_train[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #record loss values\n",
    "    train_loss.append(hist.history['loss'])\n",
    "    val_loss.append(hist.history['val_loss'])\n",
    "\n",
    "#learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.mean(train_loss, axis=0), label='Training Loss')\n",
    "plt.plot(np.mean(val_loss, axis=0), label='Validation Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "model.fit(X_train_vec,y_train,batch_size,epochs,verbose=0)\n",
    "y_pred = model.predict(X_test_vec)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tao2021",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
