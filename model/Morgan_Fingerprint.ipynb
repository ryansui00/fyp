{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import DataStructs\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset\n",
    "df = pd.read_excel(\"training_data.xlsx\")\n",
    "\n",
    "#Generate molecules\n",
    "df['ROMol'] = df.apply(lambda x: Chem.MolFromSmiles(x['SMILES']), axis=1)\n",
    "\n",
    "#Generate Morgan Fingerprints (MF) and convert RDKit object to numpy array - Source: https://github.com/rdkit/UGM_2016/blob/master/Tutorials/Part3_Fingerprints_and_classification.ipynb\n",
    "def computeMF(mol, depth=2, nBits=2048):\n",
    "    arr = np.zeros(nBits)\n",
    "    try:\n",
    "      DataStructs.ConvertToNumpyArray(AllChem.GetMorganFingerprintAsBitVect(mol,depth,nBits),arr)\n",
    "    except:\n",
    "      return None\n",
    "    return arr\n",
    "\n",
    "df['MF'] = df.apply(lambda x: computeMF(x['ROMol']),axis=1)\n",
    "\n",
    "#Generate Morgan Fingerprints with Frequency (MFF) and convert RDKit object to numpy array\n",
    "def computeMFF(mol, depth=2, nBits=2048):\n",
    "    arr = np.zeros(nBits)\n",
    "    try:\n",
    "      DataStructs.ConvertToNumpyArray(AllChem.GetHashedMorganFingerprint(mol,depth,nBits),arr)\n",
    "    except:\n",
    "      return None\n",
    "    return arr\n",
    "\n",
    "df['MFF'] = df.apply(lambda x: computeMFF(x['ROMol']),axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['SMILES','MF','MFF']],df['value'],test_size=0.2,random_state=42)\n",
    "\n",
    "X1 = np.array(X_train['MF'].values.tolist())\n",
    "X2 = np.array(X_train['MFF'].values.tolist())\n",
    "y = np.array(y_train.astype(float))\n",
    "\n",
    "X1_test = np.array(X_test['MF'].values.tolist())\n",
    "X2_test = np.array(X_test['MFF'].values.tolist())\n",
    "y_test = np.array(y_test.astype(float))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning of RF model for MF\n",
    "\n",
    "#Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "#Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt',1.0]\n",
    "#Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "#Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "#Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "#Method of selecting samples for training each tree\n",
    "bootstrap = [True, False] \n",
    "#Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "#Implement random search of parameters with 5-fold validation\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=1, random_state=0, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X1, y)\n",
    "\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement grid search hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators' : [1500,1600,1700],\n",
    "'max_features' : ['sqrt'],\n",
    "'max_depth' : [70],\n",
    "'min_samples_split' : [2],\n",
    "'min_samples_leaf' : [1],\n",
    "'bootstrap' : [False]\n",
    "}\n",
    "\n",
    "#Initialise model\n",
    "rf = RandomForestRegressor()\n",
    "#Implment grid search\n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 1)\n",
    "rf_grid.fit(X1,y)\n",
    "print(rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform 5-fold cross validation with parameters\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "for train,test in kf.split(X1,y):\n",
    "    regr = RandomForestRegressor(n_estimators = 1600, max_depth = 70, min_samples_split = 3, max_features='sqrt', min_samples_leaf = 1, bootstrap = False, random_state = 0)\n",
    "    # regr = RandomForestRegressor(n_estimators = 100, max_depth = 10, min_samples_split = 2, random_state = 0)\n",
    "    regr.fit(X1[train],y[train])\n",
    "    y_pred = regr.predict(X1[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    \n",
    "print(r2)\n",
    "print(mae)\n",
    "print(rmse)\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")\n",
    "\n",
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "regr.fit(X1,y)\n",
    "y_pred = regr.predict(X1_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning of RF model for MFF\n",
    "\n",
    "#Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "#Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt',1.0]\n",
    "#Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "#Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 3, 5]\n",
    "#Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "#Method of selecting samples for training each tree\n",
    "bootstrap = [True, False] \n",
    "#Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "#Implement random search of parameters with 5-fold validation\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=1, random_state=0, n_jobs = -1)\n",
    "#Fit the random search model\n",
    "rf_random.fit(X2, y)\n",
    "\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement grid search hyperparameter tuning\n",
    "\n",
    "param_grid = {'n_estimators' : [1700,1800,1900],\n",
    "'max_features' : ['sqrt'],\n",
    "'max_depth' : [70],\n",
    "'min_samples_split' : [2,3],\n",
    "'min_samples_leaf' : [1],\n",
    "'bootstrap' : [False]\n",
    "}\n",
    "\n",
    "#Initialise model\n",
    "rf = RandomForestRegressor()\n",
    "#Implment grid search\n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 1)\n",
    "rf_grid.fit(X2,y)\n",
    "print(rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform 5-fold cross validation with parameters\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "for train,test in kf.split(X2,y):\n",
    "    regr = RandomForestRegressor(n_estimators = 1800, max_depth = 70, min_samples_split = 2, max_features='sqrt', min_samples_leaf = 1, bootstrap = False, random_state = 0)\n",
    "    regr.fit(X2[train],y[train])\n",
    "    y_pred = regr.predict(X2[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #plot data (optional)\n",
    "    # y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "    # plt.scatter(y_true,y_pred)\n",
    "    # plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "    # plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "    # plt.xlabel(\"Actual Tg ($^\\circ$C)\")\n",
    "    # plt.ylabel(\"Predicted Tg ($^\\circ$C)\")\n",
    "    # plt.show()\n",
    "    # del regr\n",
    "\n",
    "print(r2)\n",
    "print(mae)\n",
    "print(rmse)\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "regr = RandomForestRegressor(n_estimators = 1800, max_depth = 70, min_samples_split = 2, max_features='sqrt', min_samples_leaf = 1, bootstrap = False, random_state = 0)\n",
    "regr.fit(X2,y)\n",
    "y_pred = regr.predict(X2_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "#save model\n",
    "X_comb = np.concatenate([X2,X2_test])\n",
    "y_comb = np.concatenate([y,y_test])\n",
    "regr.fit(X_comb,y_comb)\n",
    "pickle.dump(regr, open(\"Eg_C_Predictor.model\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning of SVM model for MF\n",
    "\n",
    "param_grid = {'kernel' : ['linear'],\n",
    "              'C' : [1, 10, 100],\n",
    "              'gamma' : [0.001, 0.01, 0.01,'scale','auto'],\n",
    "              'epsilon' : [0.001, 0.01, 0.1, 1]\n",
    "              }\n",
    "\n",
    "#Create model\n",
    "svr = svm.SVR()\n",
    "#Perform grid search\n",
    "svr_grid = GridSearchCV(estimator = svr, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 1)\n",
    "#Fit grid search model\n",
    "svr_grid.fit(X1,y)\n",
    "\n",
    "print(svr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further tuning of SVM model \n",
    "\n",
    "param_grid = {'kernel' : ['linear'],\n",
    "              'C' : [0.1,1,10],\n",
    "              'gamma' : [0.001, 0.01],\n",
    "              'epsilon' : [0.001,0.01]\n",
    "              }\n",
    "\n",
    "#Initialise model\n",
    "svr = svm.SVR()\n",
    "#Perform grid search\n",
    "svr_grid = GridSearchCV(estimator = svr, param_grid = param_grid, cv = 5, n_jobs = -1, verbose = 1)\n",
    "#Fit grid search model\n",
    "svr_grid.fit(X1,y)\n",
    "\n",
    "print(svr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'kernel' : ['rbf','linear','poly'],\n",
    "              'C' : [0.1,1],\n",
    "              'gamma' : [0.001],\n",
    "              'epsilon' : [0.001,0.01]\n",
    "              }\n",
    "\n",
    "#Initialise model\n",
    "svr = svm.SVR()\n",
    "#Perform grid search\n",
    "svr_grid = GridSearchCV(estimator = svr, param_grid = param_grid, cv = 5, n_jobs = -1, verbose = 1)\n",
    "#Fit grid search model\n",
    "svr_grid.fit(X1,y)\n",
    "\n",
    "print(svr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform 5-fold cross validation with parameters\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "for train,test in kf.split(X1,y):\n",
    "    regr = svm.SVR(kernel=\"linear\", C=0.1, tol=0.001, gamma=0.001,epsilon=0.1)\n",
    "    regr.fit(X1[train],y[train])\n",
    "    y_pred = regr.predict(X1[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #plot data\n",
    "    y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "    plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "    plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "    plt.annotate(\"r2 = {:.4f}\".format(r2_score(y_true, y_pred)), (7.5, 1))\n",
    "    plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "    plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "print(r2)\n",
    "print(mae)\n",
    "print(rmse)\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test model on unseen data\n",
    "regr.fit(X1,y)\n",
    "y_pred = regr.predict(X1_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "# plt.annotate(\"r2 = {:.4f}\".format(r2_score(y_true, y_pred)), (7.5, 1))\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further tuning of SVM model for MFF\n",
    "\n",
    "param_grid = {'kernel' : ['linear'],\n",
    "              'C' : [0.01,0.1,1],\n",
    "              'gamma' : [0.001, 0.01, 0.01],\n",
    "              'epsilon' : [0.001, 0.01, 0.1, 1]\n",
    "              }\n",
    "\n",
    "#Create model\n",
    "svr = svm.SVR()\n",
    "#Perform grid search\n",
    "svr_grid = GridSearchCV(estimator = svr, param_grid = param_grid, cv = 5, verbose = 1)\n",
    "#Fit grid search model\n",
    "svr_grid.fit(X2,y)\n",
    "\n",
    "print(svr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform 5-fold cross validation with parameters\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "for train,test in kf.split(X2,y):\n",
    "    regr = svm.SVR(kernel=\"linear\", C=0.05, tol=0.001, gamma=0.001,epsilon=0.1)\n",
    "    regr.fit(X2[train],y[train])\n",
    "    y_pred = regr.predict(X2[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #plot data\n",
    "    # y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "    # plt.scatter(y_true,y_pred)\n",
    "    # plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "    # plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "    # plt.xlabel(\"Actual Tg ($^\\circ$C)\")\n",
    "    # plt.ylabel(\"Predicted Tg ($^\\circ$C)\")\n",
    "    # plt.show()\n",
    "   \n",
    "\n",
    "print(r2)\n",
    "print(mae)\n",
    "print(rmse)\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test model on unseen data\n",
    "regr.fit(X2,y)\n",
    "y_pred = regr.predict(X2_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "# plt.annotate(\"r2 = {:.4f}\".format(r2_score(y_true, y_pred)), (7.5, 0.2))\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Regression (GPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import GPR model\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, RBF\n",
    "\n",
    "#initialise kernel\n",
    "kernel = WhiteKernel(noise_level=0.1) + RBF(length_scale=10)\n",
    "\n",
    "#perform 5-fold cross validation for MF\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "for train,test in kf.split(X1,y):\n",
    "    regr = GaussianProcessRegressor(kernel=kernel,random_state=0)\n",
    "    regr.fit(X1[train],y[train])\n",
    "    y_pred = regr.predict(X1[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #plot data\n",
    "    # y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "    # plt.scatter(y_true,y_pred)\n",
    "    # plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "    # plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "    # plt.xlabel(\"Actual Tg ($^\\circ$C)\")\n",
    "    # plt.ylabel(\"Predicted Tg ($^\\circ$C)\")\n",
    "    # plt.show()\n",
    "    # del regr\n",
    "\n",
    "print(r2)\n",
    "print(mae)\n",
    "print(rmse)\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "regr.fit(X1,y)\n",
    "y_pred = regr.predict(X1_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise kernel\n",
    "kernel = WhiteKernel(noise_level=0.1) + RBF(length_scale=10)\n",
    "\n",
    "#perform 5-fold cross validation for MFF\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "for train,test in kf.split(X2,y):\n",
    "    regr = GaussianProcessRegressor(kernel=kernel,random_state=0)\n",
    "    regr.fit(X2[train],y[train])\n",
    "    y_pred = regr.predict(X2[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #plot data\n",
    "    # y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "    # plt.scatter(y_true,y_pred)\n",
    "    # plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "    # plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "    # plt.xlabel(\"Actual Tg ($^\\circ$C)\")\n",
    "    # plt.ylabel(\"Predicted Tg ($^\\circ$C)\")\n",
    "    # plt.show()\n",
    "    # del regr\n",
    "\n",
    "print(r2)\n",
    "print(mae)\n",
    "print(rmse)\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "regr.fit(X2,y)\n",
    "y_pred = regr.predict(X2_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
    "\n",
    "#define parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "X1 = X1.reshape(X1.shape[0],X1.shape[1],1)\n",
    "X1_test = X1_test.reshape(X1_test.shape[0],X1_test.shape[1],1) \n",
    "\n",
    "#implement model for MF\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(2048, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "#compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#run k-cross validation\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "train_loss, val_loss = [], []\n",
    "for train,test in kf.split(X1,y):\n",
    "    hist = model.fit(X1[train],y[train],batch_size,epochs,verbose=0,validation_data=(X1[test], y[test]))\n",
    "    y_pred = model.predict(X1[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #record loss values\n",
    "    train_loss.append(hist.history['loss'])\n",
    "    val_loss.append(hist.history['val_loss'])\n",
    "\n",
    "#learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.mean(train_loss, axis=0), label='Training Loss')\n",
    "plt.plot(np.mean(val_loss, axis=0), label='Validation Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "model.fit(X1,y,batch_size,epochs,verbose=0)\n",
    "y_pred = model.predict(X1_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
    "\n",
    "#define parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "X2 = X2.reshape(X2.shape[0],X2.shape[1],1)\n",
    "X2_test = X2_test.reshape(X2_test.shape[0],X2_test.shape[1],1)\n",
    "\n",
    "#implement model for MFF\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(2048, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "#compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#run k-cross validation\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "train_loss, val_loss = [], []\n",
    "for train,test in kf.split(X2,y):\n",
    "    hist = model.fit(X2[train],y[train],batch_size,epochs,verbose=0,validation_data=(X2[test], y[test]))\n",
    "    y_pred = model.predict(X2[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #record loss values\n",
    "    train_loss.append(hist.history['loss'])\n",
    "    val_loss.append(hist.history['val_loss'])\n",
    "\n",
    "#learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.mean(train_loss, axis=0), label='Training Loss')\n",
    "plt.plot(np.mean(val_loss, axis=0), label='Validation Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "model.fit(X2,y,batch_size,epochs,verbose=0)\n",
    "y_pred = model.predict(X2_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "#define parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "X1 = X1.reshape(X1.shape[0],1,X1.shape[1])\n",
    "X1_test = X1_test.reshape(X1_test.shape[0],1,X1_test.shape[1]) \n",
    "\n",
    "#implment model for MF\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1, 2048)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "#compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#run k-cross validation\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "train_loss, val_loss = [], []\n",
    "for train,test in kf.split(X1,y):\n",
    "    hist = model.fit(X1[train],y[train],batch_size,epochs,verbose=0,validation_data=(X1[test], y[test]))\n",
    "    y_pred = model.predict(X1[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #record loss values\n",
    "    train_loss.append(hist.history['loss'])\n",
    "    val_loss.append(hist.history['val_loss'])\n",
    "\n",
    "#learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.mean(train_loss, axis=0), label='Training Loss')\n",
    "plt.plot(np.mean(val_loss, axis=0), label='Validation Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "model.fit(X1,y,batch_size,epochs,verbose=0)\n",
    "y_pred = model.predict(X1_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "#define parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "X2 = X2.reshape(X2.shape[0],1,X2.shape[1])\n",
    "X2_test = X2_test.reshape(X2_test.shape[0],1,X2_test.shape[1]) \n",
    "\n",
    "#implment model for MFF\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1, 2048)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "#compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#run k-cross validation\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "train_loss, val_loss = [], []\n",
    "for train,test in kf.split(X2,y):\n",
    "    hist = model.fit(X2[train],y[train],batch_size,epochs,verbose=0,validation_data=(X2[test], y[test]))\n",
    "    y_pred = model.predict(X2[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #record loss values\n",
    "    train_loss.append(hist.history['loss'])\n",
    "    val_loss.append(hist.history['val_loss'])\n",
    "\n",
    "#learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.mean(train_loss, axis=0), label='Training Loss')\n",
    "plt.plot(np.mean(val_loss, axis=0), label='Validation Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "model.fit(X2,y,batch_size,epochs,verbose=0)\n",
    "y_pred = model.predict(X2_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred,s=10,alpha=0.9)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black',alpha=0.7)\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN (with Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "#define parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "X2 = X2.reshape(X2.shape[0],X2.shape[1],1)\n",
    "X2_test = X2_test.reshape(X2_test.shape[0],X2_test.shape[1],1)\n",
    "\n",
    "#implement model for MFF\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(2048, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))  # Adding dropout layer\n",
    "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))  # Adding dropout layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))   # Adding dropout layer\n",
    "model.add(Dense(1, activation='linear'))\n",
    "#compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#run k-cross validation\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "train_loss, val_loss = [], []\n",
    "for train,test in kf.split(X1,y):\n",
    "    hist = model.fit(X1[train],y[train],batch_size,epochs,verbose=0,validation_data=(X2[test], y[test]))\n",
    "    y_pred = model.predict(X1[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #record loss values\n",
    "    train_loss.append(hist.history['loss'])\n",
    "    val_loss.append(hist.history['val_loss'])\n",
    "\n",
    "#learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.mean(train_loss, axis=0), label='Training Loss')\n",
    "plt.plot(np.mean(val_loss, axis=0), label='Validation Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "model.fit(X2,y,batch_size,epochs,verbose=0)\n",
    "y_pred = model.predict(X2_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
