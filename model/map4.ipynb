{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tmap as tm\n",
    "from map4 import MAP4Calculator\n",
    "\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import DataStructs\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset\n",
    "df = pd.read_excel(\"training_data.xlsx\",engine='openpyxl')\n",
    "\n",
    "#Generate molecules\n",
    "df['ROMol'] = df.apply(lambda x: Chem.MolFromSmiles(x['SMILES']), axis=1)\n",
    "# print(df.head())\n",
    "\n",
    "#Generate MAP4 from RDKit object\n",
    "dim = 1024\n",
    "MAP4 = MAP4Calculator(dimensions=dim)\n",
    "# ENC = tm.MinHash(dim)\n",
    "df['MAP4'] = df.apply(lambda x: MAP4.calculate(x['ROMol']), axis=1)\n",
    "# print(df.head())\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(df[['SMILES','MAP4']],df['value'], test_size=0.2,random_state=420)\n",
    "\n",
    "X = np.array(X_train['MAP4'].values.tolist())\n",
    "y = np.array(y_train.astype(float))\n",
    "# print(X)\n",
    "\n",
    "X_test = np.array(X_test['MAP4'].values.tolist())\n",
    "y_test = np.array(y_test.astype(float))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning of RF model for MF\n",
    "\n",
    "# umber of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "#Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt',1.0]\n",
    "#Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "#Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "#Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "#Sampling method of selecting samples for training each tree\n",
    "bootstrap = [True, False] \n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "#Implement random search of parameters with 5-fold validation\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=0, n_jobs = -1)\n",
    "#Fit the random search model\n",
    "rf_random.fit(X, y)\n",
    " \n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement grid search hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators' : [1600,1700,1800],\n",
    "'max_features' : ['sqrt'],\n",
    "'max_depth' : [70],\n",
    "'min_samples_split' : [1,2,3],\n",
    "'min_samples_leaf' : [1],\n",
    "'bootstrap' : [False]\n",
    "}\n",
    "\n",
    "#Initialise model\n",
    "rf = RandomForestRegressor()\n",
    "#Implment grid search\n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 1)\n",
    "rf_grid.fit(X,y)\n",
    "print(rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform 5-fold cross validation with parameters\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "for train,test in kf.split(X,y):\n",
    "    regr = RandomForestRegressor(n_estimators = 1700, max_depth = 70, min_samples_split = 2, max_features='sqrt', min_samples_leaf = 1, bootstrap = False, random_state = 420)\n",
    "    # regr = RandomForestRegressor(n_estimators = 100, max_depth = 10, min_samples_split = 2, random_state = 0)\n",
    "    regr.fit(X[train],y[train])\n",
    "    y_pred = regr.predict(X[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #plot data (optional)\n",
    "    # y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "    # plt.scatter(y_true,y_pred)\n",
    "    # plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "    # plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "    # plt.xlabel(\"Actual Tg ($^\\circ$C)\")\n",
    "    # plt.ylabel(\"Predicted Tg ($^\\circ$C)\")\n",
    "    # plt.show()\n",
    "    # del regr\n",
    "\n",
    "print(r2)\n",
    "print(mae)\n",
    "print(rmse)\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "regr = RandomForestRegressor(n_estimators = 1700, max_depth = 70, min_samples_split = 2, max_features='sqrt', min_samples_leaf = 1, bootstrap = False, random_state = 420)\n",
    "regr.fit(X,y)\n",
    "y_pred = regr.predict(X_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "pickle.dump(regr, open(\"RF_MAP4.model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "# plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()\n",
    "del regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "#scale data for SVM\n",
    "scaler = MaxAbsScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning of SVM model for MF\n",
    "\n",
    "param_grid = {'kernel' : ['linear'],\n",
    "              'C' : [0.1, 1, 10],\n",
    "              'gamma' : [0.001, 0.01, 0.01,'scale','auto'],\n",
    "              'epsilon' : [0.001, 0.01, 0.1, 1]\n",
    "              }\n",
    "\n",
    "#Create model\n",
    "svr = svm.SVR()\n",
    "#Perform grid search\n",
    "svr_grid = GridSearchCV(estimator = svr, param_grid = param_grid, cv = 3, n_jobs = -1 , verbose = 1)\n",
    "#Fit grid search model\n",
    "svr_grid.fit(X_scaled,y)\n",
    "\n",
    "print(svr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further tuning of SVM model \n",
    "\n",
    "param_grid = {'kernel' : ['linear','poly','rbf','sigmoid'],\n",
    "              'C' : [0.1,1, 10],\n",
    "              'gamma' : [0.001, 0.01, 0.01,'scale','auto'],\n",
    "              'epsilon' : [0.1, 1, 10]\n",
    "              }\n",
    "\n",
    "#Initialise model\n",
    "svr = svm.SVR()\n",
    "#Perform grid search\n",
    "svr_grid = GridSearchCV(estimator = svr, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
    "#Fit grid search model\n",
    "svr_grid.fit(X_scaled,y)\n",
    "\n",
    "print(svr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform 5-fold cross validation with parameters\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "for train,test in kf.split(X_scaled,y):\n",
    "    regr = svm.SVR(kernel=\"rbf\", C=10, tol=0.001, gamma='scale',epsilon=0.1)\n",
    "    regr.fit(X_scaled[train],y[train])\n",
    "    y_pred = regr.predict(X_scaled[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #plot data (optional)\n",
    "    # y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "    # plt.scatter(y_true,y_pred)\n",
    "    # plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "    # plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "    # plt.xlabel(\"Actual Tg ($^\\circ$C)\")\n",
    "    # plt.ylabel(\"Predicted Tg ($^\\circ$C)\")\n",
    "    # plt.show()\n",
    "   \n",
    "\n",
    "print(r2)\n",
    "print(mae)\n",
    "print(rmse)\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "regr = svm.SVR(kernel=\"rbf\", C=10, tol=0.001, gamma='scale',epsilon=0.1)\n",
    "regr.fit(X_scaled,y)\n",
    "y_pred = regr.predict(X_test_scaled)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "pickle.dump(regr, open(\"SVM_MAP4.model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "# plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()\n",
    "del regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Regression (GPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import GPR model\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, RBF\n",
    "\n",
    "#initialise kernel\n",
    "kernel = WhiteKernel(noise_level=1.0) + RBF(length_scale=10)\n",
    "\n",
    "#perform 5-fold cross validation for MF\n",
    "kf = KFold(n_splits=5)\n",
    "r2, mae, rmse = [], [], []\n",
    "for train,test in kf.split(X_scaled,y):\n",
    "    regr = GaussianProcessRegressor(kernel=kernel,random_state=0)\n",
    "    regr.fit(X_scaled[train],y[train]) \n",
    "    y_pred = regr.predict(X_scaled[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2.append(r2_score(y_true,y_pred))\n",
    "    mae.append(mean_absolute_error(y_true,y_pred))\n",
    "    rmse.append(mean_squared_error(y_true,y_pred,squared=False))\n",
    "\n",
    "    #plot data\n",
    "    # y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "    # plt.scatter(y_true,y_pred)\n",
    "    # plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "    # plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "    # plt.xlabel(\"Actual Tg ($^\\circ$C)\")\n",
    "    # plt.ylabel(\"Predicted Tg ($^\\circ$C)\")\n",
    "    # plt.show()\n",
    "    # del regr\n",
    "\n",
    "print(r2)\n",
    "print(mae)\n",
    "print(rmse)\n",
    "print(f\"Average r2 score: {np.mean(r2)}\\nAverage MAE: {np.mean(mae)}\\nAverage RMSE: {np.mean(rmse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "kernel = WhiteKernel(noise_level=1.0) + RBF(length_scale=10)\n",
    "regr = GaussianProcessRegressor(kernel,random_state=420,normalize_y=True,n_restarts_optimizer=5)\n",
    "regr.fit(X_scaled,y)\n",
    "y_pred = regr.predict(X_test_scaled)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "pickle.dump(regr, open(\"GPR_MAP4.model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "# plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()\n",
    "del regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
    "\n",
    "#define parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "X = X.reshape(X.shape[0],X.shape[1],1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],1) \n",
    "\n",
    "#implement model for MF\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(1024, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "#compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error',metrics=['accuracy'])\n",
    "\n",
    "#run k-cross validation\n",
    "r2_scores, mae_scores, rmse_scores = [], [], []\n",
    "train_loss, val_loss = [], []\n",
    "train_acc, val_acc = [], []\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(X, y):\n",
    "    hist = model.fit(X[train], y[train], batch_size, epochs, verbose=0, validation_data=(X[test], y[test]))\n",
    "    \n",
    "    # evaluate model performance\n",
    "    y_pred = model.predict(X[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2_scores.append(r2_score(y_true, y_pred))\n",
    "    mae_scores.append(mean_absolute_error(y_true, y_pred))\n",
    "    rmse_scores.append(mean_squared_error(y_true, y_pred, squared=False))\n",
    "\n",
    "    # record loss and accuracy values\n",
    "    train_loss.append(hist.history['loss'])\n",
    "    val_loss.append(hist.history['val_loss'])\n",
    "    train_acc.append(hist.history['accuracy'])\n",
    "    val_acc.append(hist.history['val_accuracy'])\n",
    "\n",
    "#plot learning curve\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "#plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.mean(train_loss, axis=0), label='Training Loss')\n",
    "plt.plot(np.mean(val_loss, axis=0), label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "#plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.mean(train_acc, axis=0), label='Training Accuracy')\n",
    "plt.plot(np.mean(val_acc, axis=0), label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#print average metrics\n",
    "print(f\"Average r2 score: {np.mean(r2_scores)}\\nAverage MAE: {np.mean(mae_scores)}\\nAverage RMSE: {np.mean(rmse_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "model.fit(X,y,batch_size,epochs,verbose=0)\n",
    "y_pred = model.predict(X_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "pickle.dump(regr, open(\"CNN_MAP4.model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "# plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "#define parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "X = X.reshape(X.shape[0],1,X.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0],1,X_test.shape[1]) \n",
    "\n",
    "#implment model for MF\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1, 1024)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "#compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error',metrics=['accuracy'])\n",
    "\n",
    "#run k-cross validation\n",
    "r2_scores, mae_scores, rmse_scores = [], [], []\n",
    "train_loss, val_loss = [], []\n",
    "train_acc, val_acc = [], []\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(X, y):\n",
    "    hist = model.fit(X[train], y[train], batch_size, epochs, verbose=0, validation_data=(X[test], y[test]))\n",
    "    \n",
    "    # evaluate model performance\n",
    "    y_pred = model.predict(X[test])\n",
    "    y_true = np.array(y[test])\n",
    "    r2_scores.append(r2_score(y_true, y_pred))\n",
    "    mae_scores.append(mean_absolute_error(y_true, y_pred))\n",
    "    rmse_scores.append(mean_squared_error(y_true, y_pred, squared=False))\n",
    "\n",
    "    # record loss and accuracy values\n",
    "    train_loss.append(hist.history['loss'])\n",
    "    val_loss.append(hist.history['val_loss'])\n",
    "    train_acc.append(hist.history['accuracy'])\n",
    "    val_acc.append(hist.history['val_accuracy'])\n",
    "\n",
    "#plot training and validation loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.mean(train_loss, axis=0), label='Training Loss')\n",
    "plt.plot(np.mean(val_loss, axis=0), label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# plot training and validation accuracy\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.mean(train_acc, axis=0), label='Training Accuracy')\n",
    "plt.plot(np.mean(val_acc, axis=0), label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print average metrics\n",
    "print(f\"Average r2 score: {np.mean(r2_scores)}\\nAverage MAE: {np.mean(mae_scores)}\\nAverage RMSE: {np.mean(rmse_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against unseen data\n",
    "model.fit(X,y,batch_size,epochs,verbose=0)\n",
    "y_pred = model.predict(X_test)\n",
    "y_true = np.array(y_test)\n",
    "\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "pickle.dump(regr, open(\"RNN_MAP4.model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data\n",
    "y_true, y_pred = y_true.reshape(-1,1), y_pred.reshape(-1,1)\n",
    "plt.scatter(y_true,y_pred)\n",
    "plt.plot(y_true, LinearRegression().fit(y_true, y_pred).predict(y_true),color='black')\n",
    "# plt.annotate(\"r2 = {:.3f}\".format(r2_score(y_true, y_pred)), (200, -50))\n",
    "plt.xlabel(\"Actual Bandgap (eV)\")\n",
    "plt.ylabel(\"Predicted Bandgap (eV)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "map4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
